{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Performance Results Analysis\n",
    "\n",
    "This notebook reads all *_summary.json files from the result_outputs folder and loads them into a pandas DataFrame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the result_outputs folder\n",
    "results_path = Path('result_outputs')\n",
    "\n",
    "# Find all files ending with _summary.json\n",
    "summary_files = list(results_path.glob('*_summary.json'))\n",
    "print(f\"Found {len(summary_files)} summary files:\")\n",
    "for file in summary_files:\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and parse JSON files\n",
    "def read_summary_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all summary files and collect data\n",
    "all_data = []\n",
    "\n",
    "for file_path in summary_files:\n",
    "    try:\n",
    "        data = read_summary_json(file_path)\n",
    "        # Add filename for reference\n",
    "        data['filename'] = file_path.name\n",
    "        all_data.append(data)\n",
    "        print(f\"Successfully loaded: {file_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path.name}: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(all_data)} files successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "## drop columns that are not needed 'version'\n",
    "df.drop(columns=['version'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up model names - remove prefix up to '/'\n",
    "df['model'] = df['model'].str.split('/').str[-1]\n",
    "print(\"Cleaned model names:\")\n",
    "print(df['model'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info about the DataFrame\n",
    "print(\"DataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to CSV file in result_outputs folder\n",
    "csv_filename = 'result_outputs/llm_performance_results.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "print(f\"DataFrame saved to {csv_filename}\")\n",
    "print(f\"Saved {len(df)} rows and {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Time to First Token (TTFT) by model\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_sorted_ttft = df.sort_values('results_ttft_s_mean', ascending=True)\n",
    "\n",
    "sns.barplot(data=df_sorted_ttft, x='model', y='results_ttft_s_mean', hue='model', palette='plasma', legend=False)\n",
    "plt.title('Time to First Token (TTFT) by Model (Sorted Low to High, Lower is Better)')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Mean TTFT (seconds)')\n",
    "plt.xticks(rotation=60, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a bar graph of 'model' vs 'results_num_completed_requests_per_min', sorted by throughput (low to high)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sort the DataFrame by throughput\n",
    "df_sorted = df.sort_values('results_num_completed_requests_per_min', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_sorted, x='model', y='results_num_completed_requests_per_min', hue='model', palette='viridis', legend=False)\n",
    "plt.title('Throughput: Completed Requests per Minute by Model (Sorted Low to High, Higher is Better)')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Completed Requests per Minute')\n",
    "plt.xticks(rotation=60, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive latency comparison chart\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Inter-token Latency\n",
    "df_sorted_inter = df.sort_values('results_inter_token_latency_s_mean', ascending=True)\n",
    "sns.barplot(data=df_sorted_inter, x='model', y='results_inter_token_latency_s_mean', \n",
    "           hue='model', palette='viridis', legend=False, ax=axes[0,0])\n",
    "axes[0,0].set_title('Inter-token Latency by Model (Lower is Better)')\n",
    "axes[0,0].set_xlabel('Model')\n",
    "axes[0,0].set_ylabel('Mean Inter-token Latency (seconds)')\n",
    "axes[0,0].tick_params(axis='x', rotation=60, labelsize=9)\n",
    "\n",
    "# 2. End-to-End Latency\n",
    "df_sorted_e2e = df.sort_values('results_end_to_end_latency_s_mean', ascending=True)\n",
    "sns.barplot(data=df_sorted_e2e, x='model', y='results_end_to_end_latency_s_mean', \n",
    "           hue='model', palette='plasma', legend=False, ax=axes[0,1])\n",
    "axes[0,1].set_title('End-to-End Latency by Model (Lower is Better)')\n",
    "axes[0,1].set_xlabel('Model')\n",
    "axes[0,1].set_ylabel('Mean End-to-End Latency (seconds)')\n",
    "axes[0,1].tick_params(axis='x', rotation=60, labelsize=9)\n",
    "\n",
    "# 3. Latency Distribution (Box plot for TTFT)\n",
    "sns.boxplot(data=df.melt(id_vars=['model'], \n",
    "                        value_vars=['results_ttft_s_quantiles_p25', 'results_ttft_s_quantiles_p50', \n",
    "                                   'results_ttft_s_quantiles_p75', 'results_ttft_s_quantiles_p90']),\n",
    "           x='model', y='value', hue='model', palette='Set2', legend=False, ax=axes[1,0])\n",
    "axes[1,0].set_title('TTFT Distribution (P25, P50, P75, P90)')\n",
    "axes[1,0].set_xlabel('Model')\n",
    "axes[1,0].set_ylabel('TTFT (seconds)')\n",
    "axes[1,0].tick_params(axis='x', rotation=60, labelsize=9)\n",
    "\n",
    "# 4. Combined Latency Comparison\n",
    "latency_metrics = ['results_ttft_s_mean', 'results_inter_token_latency_s_mean', 'results_end_to_end_latency_s_mean']\n",
    "df_melted = df.melt(id_vars=['model'], value_vars=latency_metrics, \n",
    "                   var_name='latency_type', value_name='latency_seconds')\n",
    "df_melted['latency_type'] = df_melted['latency_type'].str.replace('results_', '').str.replace('_s_mean', '').str.replace('_', ' ').str.title()\n",
    "\n",
    "sns.barplot(data=df_melted, x='model', y='latency_seconds', hue='latency_type', ax=axes[1,1])\n",
    "axes[1,1].set_title('Latency Metrics Comparison')\n",
    "axes[1,1].set_xlabel('Model')\n",
    "axes[1,1].set_ylabel('Latency (seconds)')\n",
    "axes[1,1].tick_params(axis='x', rotation=60, labelsize=9)\n",
    "axes[1,1].legend(title='Latency Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive throughput analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Overall Output Throughput (tokens/sec)\n",
    "df_sorted_throughput = df.sort_values('results_mean_output_throughput_token_per_s', ascending=False)\n",
    "sns.barplot(data=df_sorted_throughput, x='model', y='results_mean_output_throughput_token_per_s', \n",
    "           hue='model', palette='viridis', legend=False, ax=axes[0,0])\n",
    "axes[0,0].set_title('Overall Output Throughput by Model (Higher is Better)')\n",
    "axes[0,0].set_xlabel('Model')\n",
    "axes[0,0].set_ylabel('Mean Output Throughput (tokens/sec)')\n",
    "axes[0,0].tick_params(axis='x', rotation=60, labelsize=9)\n",
    "\n",
    "# 2. Request Throughput (requests/min)\n",
    "df_sorted_requests = df.sort_values('results_num_completed_requests_per_min', ascending=False)\n",
    "sns.barplot(data=df_sorted_requests, x='model', y='results_num_completed_requests_per_min', \n",
    "           hue='model', palette='plasma', legend=False, ax=axes[0,1])\n",
    "axes[0,1].set_title('Request Throughput by Model (Higher is Better)')\n",
    "axes[0,1].set_xlabel('Model')\n",
    "axes[0,1].set_ylabel('Completed Requests per Minute')\n",
    "axes[0,1].tick_params(axis='x', rotation=60, labelsize=9)\n",
    "\n",
    "# 3. Per-Request Output Throughput Distribution\n",
    "sns.boxplot(data=df.melt(id_vars=['model'], \n",
    "                        value_vars=['results_request_output_throughput_token_per_s_quantiles_p25',\n",
    "                                   'results_request_output_throughput_token_per_s_quantiles_p50',\n",
    "                                   'results_request_output_throughput_token_per_s_quantiles_p75',\n",
    "                                   'results_request_output_throughput_token_per_s_quantiles_p90']),\n",
    "           x='model', y='value', hue='model', palette='tab10', legend=False, ax=axes[1,0])\n",
    "axes[1,0].set_title('Per-Request Throughput Distribution (P25, P50, P75, P90)')\n",
    "axes[1,0].set_xlabel('Model')\n",
    "axes[1,0].set_ylabel('Per-Request Throughput (tokens/sec)')\n",
    "axes[1,0].tick_params(axis='x', rotation=60, labelsize=9)\n",
    "\n",
    "# 4. Throughput vs Latency Scatter Plot\n",
    "axes[1,1].scatter(df['results_ttft_s_mean'], df['results_mean_output_throughput_token_per_s'], \n",
    "                 s=100, alpha=0.7, c=range(len(df)), cmap='viridis')\n",
    "axes[1,1].set_xlabel('Mean TTFT (seconds)')\n",
    "axes[1,1].set_ylabel('Mean Output Throughput (tokens/sec)')\n",
    "axes[1,1].set_title('Throughput vs TTFT Trade-off')\n",
    "\n",
    "# Add model labels to scatter plot\n",
    "for i, model in enumerate(df['model']):\n",
    "    axes[1,1].annotate(model.split('/')[-1], \n",
    "                      (df.iloc[i]['results_ttft_s_mean'], \n",
    "                       df.iloc[i]['results_mean_output_throughput_token_per_s']),\n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=8, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save it a HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!!jupyter nbconvert  --to HTML *.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmperf-nebius-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
